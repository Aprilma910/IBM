{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.16.2)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m188.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m148.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 07:30:44.847046: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-11 07:30:44.848053: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-11 07:30:44.851523: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-11 07:30:44.863143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-11 07:30:44.885867: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-11 07:30:44.885921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 07:30:44.902058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-11 07:30:45.934105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - loss: 13.7763 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.2195\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.1560 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1649   \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - loss: 0.1569 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1291 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1331 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1460\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1032 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1307 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0857 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0827    \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1109 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1080 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0656    \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0644 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0562 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0394    \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0339   \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.0349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fe5c81753d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 309ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 5.0976  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 1.1583 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.6330 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.2269 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1234 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - loss: 0.0687\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0430 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0345\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0268   \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0236 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0197 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0227 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0187 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0168 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0159 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0151 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0146 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0135 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0132 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0129\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 299ms/step - loss: 0.0011    \n",
      "Test loss: 0.001930752769112587\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(0.5)(flatten) \n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "# Build the model \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Train the model \n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "# Evaluate the model \n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 605ms/step - loss: 0.0205 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 598ms/step - loss: 0.0233 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 590ms/step - loss: 0.0203 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 589ms/step - loss: 0.0436 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 592ms/step - loss: 0.0202 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 600ms/step - loss: 0.0289 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 599ms/step - loss: 0.0412 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 598ms/step - loss: 0.0238 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 595ms/step - loss: 0.0149 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 597ms/step - loss: 0.0122 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 601ms/step - loss: 0.0140 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 591ms/step - loss: 0.0136 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 594ms/step - loss: 0.0142 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 599ms/step - loss: 0.0110 \n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 591ms/step - loss: 0.0177 \n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 597ms/step - loss: 0.0188 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 595ms/step - loss: 0.0122 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 599ms/step - loss: 0.0114 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 600ms/step - loss: 0.0109 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 603ms/step - loss: 0.0110 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 301ms/step - loss: 6.0340e-04\n",
      "Test loss with batch size 16: 0.002182916272431612\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0108 \n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0045\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0039\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0037\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0034\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0034\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0029\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0029\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0031\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 0.0034   \n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0032\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0036\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2s/step - loss: 0.0030\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0027\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0034\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0034\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0037\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0026\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 299ms/step - loss: 4.9725e-04\n",
      "Test loss with batch size 64: 0.0004068459675181657\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m19/60\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 0.2777 "
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
